{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from textstat.textstat import textstat\n",
    "from textblob import TextBlob\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class PorterTokenizer(object):\n",
    "    \"\"\"Custom PorterTokenizer for TfidfVectorizer\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        translate_table = dict((ord(char), None) for char in string.punctuation)\n",
    "        return [self.stemmer.stem(t) for t in word_tokenize(doc.translate(translate_table))]\n",
    "\n",
    "class Vectorizer(object):\n",
    "    \"\"\"Vecotizer wrapper for sklearn TfidfVectorizer.\n",
    "\n",
    "    Allows passing of custom tokenizer\n",
    "\n",
    "    TODO: add more custom tokenizers\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 tokenizer=None,\n",
    "                 encoding='utf-8',\n",
    "                 stop_words='english',\n",
    "                 min_df=1,\n",
    "                 ngram_range=None):\n",
    "        self.tokenizers = {'porter': PorterTokenizer()}\n",
    "        self.vectorizer = TfidfVectorizer(tokenizer=self.tokenizers[tokenizer],\n",
    "                                          encoding=encoding,\n",
    "                                          stop_words=stop_words,\n",
    "                                          min_df=min_df,\n",
    "                                          ngram_range=ngram_range)\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.vectorizer.fit(X)\n",
    "        return self\n",
    "\n",
    "    def fit_transform(self, X):\n",
    "        return self.vectorizer.fit_transform(X)\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.vectorizer.transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# feature extraction\n",
    "def remove_handles(content):\n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)\",\" \",content).split())\n",
    "\n",
    "def count_handles(content):\n",
    "    return len(re.findall(\"(@[A-Za-z0-9]+)\",content))\n",
    "\n",
    "def bool_handles(content):\n",
    "    match = re.search(\"(@[A-Za-z0-9]+)\", content)\n",
    "    if match:\n",
    "        return 1\n",
    "    else: return 0\n",
    "\n",
    "def count_hashtags(content):\n",
    "    return len(re.findall(\"(#[A-Za-z0-9]+)\",content))\n",
    "\n",
    "def bool_hashtags(content):\n",
    "    match = re.search(\"(#[A-Za-z0-9]+)\", content)\n",
    "    if match:\n",
    "        return 1\n",
    "    else: return 0\n",
    "\n",
    "def is_retweet(content):\n",
    "    return int(\"RT \" in content)\n",
    "\n",
    "def has_url(content):\n",
    "    return int(\"https://\" in content or \"http://\" in content)\n",
    "\n",
    "def build_POS_list(content):\n",
    "    content = content.decode('latin-1')\n",
    "    return ' '.join([item[1] for item in pos_tag(word_tokenize(content))])\n",
    "\n",
    "def create_features(df, feature_cols, vec, pos_vectorizer):\n",
    "    features = df[feature_cols].values\n",
    "    n_gram_vector = vec.vectorizer.transform(df['tweet_no_handle'].values)\n",
    "    pos_ngram_vector = pos_vectorizer.transform(df['pos_tags'].values)\n",
    "    feature_vector = np.concatenate((n_gram_vector.todense(), features, pos_ngram_vector.todense()), axis=1)\n",
    "    \n",
    "    return feature_vector\n",
    "\n",
    "# Model specifications for tweets\n",
    "def test_model(base_model, param_grid):\n",
    "    grid_clf = GridSearchCV(base_model, param_grid, cv=5)\n",
    "    grid_clf.fit(train_features, y_train)\n",
    "    preds = grid_clf.predict(test_features)\n",
    "    print(classification_report(y_test, preds))\n",
    "    return grid_clf\n",
    "\n",
    "def top_words(clf, label, top):\n",
    "    data = []\n",
    "    for i in clf.best_estimator_.coef_[label, :].argsort()[::-1][:top]:\n",
    "        top_words = (i, clf.best_estimator_.coef_[0, i], vec.vectorizer.get_feature_names()[i])\n",
    "        return_data.append(\"{}\".format(top_words[2]))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_only_df = pd.read_csv('data/labels_and_text_only.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Features/ Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "text_only_df['tweet_no_handle'] = text_only_df['tweet_text'].apply(remove_handles)\n",
    "text_only_df['reading_ease'] = text_only_df['tweet_no_handle'].apply(textstat.flesch_reading_ease)\n",
    "text_only_df['reading_grade'] = text_only_df['tweet_no_handle'].apply(textstat.flesch_kincaid_grade)\n",
    "text_only_df['sentiment'] = text_only_df['tweet_no_handle'].map(lambda x: TextBlob(x.decode('latin-1')).polarity)\n",
    "text_only_df['subjectivity'] = text_only_df['tweet_no_handle'].map(lambda x: TextBlob(x.decode('latin-1')).subjectivity)\n",
    "text_only_df['mentions_count'] = text_only_df['tweet_text'].apply(count_handles)\n",
    "text_only_df['mentions_bool'] = text_only_df['tweet_text'].apply(bool_handles)\n",
    "text_only_df['hashtag_count'] = text_only_df['tweet_text'].apply(count_hashtags)\n",
    "text_only_df['hashtag_bool'] = text_only_df['tweet_text'].apply(bool_hashtags)\n",
    "text_only_df['has_url'] = text_only_df['tweet_text'].apply(is_retweet)\n",
    "text_only_df['tweet_length'] = text_only_df['tweet_no_handle'].apply(len)\n",
    "text_only_df['word_count'] = text_only_df['tweet_no_handle'].apply(textstat.lexicon_count)\n",
    "text_only_df['syllable_count'] = text_only_df['tweet_no_handle'].apply(textstat.syllable_count)\n",
    "text_only_df['pos_tags'] = text_only_df['tweet_no_handle'].apply(build_POS_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Determine logisitic model with x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = text_only_df.drop(['tweet_text', 'labels'], axis=1)\n",
    "y = text_only_df['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "vec = Vectorizer(tokenizer='porter',\n",
    "                   encoding='latin-1',\n",
    "                   min_df=5,\n",
    "                   ngram_range=(1,3))\n",
    "pos_vectorizer = CountVectorizer(ngram_range=(1,3), min_df=5)\n",
    "text_only_df['pos_tag_ngrams'] = pos_vectorizer.fit_transform(text_only_df['pos_tags']).todense().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#text_only_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_cols = [u'reading_ease',\n",
    "                u'reading_grade', \n",
    "                u'sentiment', \n",
    "                u'subjectivity', \n",
    "                u'mentions_count',\n",
    "                u'mentions_bool', \n",
    "                u'hashtag_count', \n",
    "                u'hashtag_bool', \n",
    "                u'has_url',\n",
    "                u'tweet_length', \n",
    "                u'word_count', \n",
    "                u'syllable_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_text = X_train['tweet_no_handle'] \n",
    "vec.fit(train_text);\n",
    "pos_vectorizer.fit(X_train['pos_tags'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "train_features = create_features(X_train, feature_cols, vec, pos_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_features = create_features(X_test, feature_cols, vec, pos_vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(class_weight='balanced', C=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "lr_model.fit(train_features, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000]}\n",
    "]\n",
    "\n",
    "log_r = LogisticRegression(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "my_model = test_model(log_r, param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Dump the model and best fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nigger\n",
      "faggot\n",
      "queer\n",
      "wetback\n",
      "kill\n",
      "fag\n",
      "monkey\n",
      "dyke\n",
      "faggot like\n",
      "sand nigger\n",
      "retard\n",
      "littl faggot\n",
      "nigga\n",
      "white trash\n",
      "jew\n",
      "homo\n",
      "towel head\n",
      "like faggot\n",
      "faggot ass\n",
      "spic\n",
      "porch\n",
      "faggot bitch\n",
      "muzzi\n",
      "trump\n",
      "ur\n",
      "dick\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-56-0dd9b008d844>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtwitter_top_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-a4fe81d3f37b>\u001b[0m in \u001b[0;36mtop_words\u001b[0;34m(clf, label, top)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtop_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mtop_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m\"{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_words\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "twitter_top_words = top_words(my_model, 0, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/hate-speech-classifier.pkl']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(my_model, 'model/hate-speech-classifier.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model/hate-speech-vector.pkl']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(vec, 'model/hate-speech-vector.pkl') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
